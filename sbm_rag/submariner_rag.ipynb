{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
    "from pathlib import Path\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import Document as LlamaDocument\n",
    "from llama_index.core.node_parser import SentenceSplitter \n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as datetime\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open-source models (Mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM \n",
    "llm = MistralAI(api_key=MISTRAL_API_KEY, model=\"mistral-large-latest\")\n",
    "\n",
    "# # Embedings model\n",
    "embed = MistralAIEmbedding(api_key=MISTRAL_API_KEY, model_name=\"mistral-embed\")\n",
    "\n",
    "# Instruct model\n",
    "instruct = MistralAI(api_key=MISTRAL_API_KEY, model=\"codestral-latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom class for documents\n",
    "# nodes creations expects get_content method\n",
    "\n",
    "# class Document(LlamaDocument):\n",
    "#     def __init__(self, content, metadata, *embedding):\n",
    "#         self.content = content\n",
    "#         self.metadata = metadata\n",
    "#         self.embedding = embedding\n",
    "\n",
    "#     def get_content(self, metadata_mode=None):\n",
    "#         content = self.content\n",
    "#         return str(content)\n",
    "    \n",
    "#     def get_metadata(self):\n",
    "#         metadata = self.metadata\n",
    "#         return str(metadata)\n",
    "    \n",
    "#     def to_dict(self):\n",
    "#         return {\"content\": self.content, \"metadata\": self.metadata}\n",
    "    \n",
    "#     def get_type(self):\n",
    "#         return type(self)\n",
    "    \n",
    "#     def get_embedding(self):\n",
    "#         return self.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(input_dir=\"data\") # For text files\n",
    "\n",
    "data_folder_path = Path(\"data\")\n",
    "\n",
    "all_documents = []\n",
    "\n",
    "for csv_file in data_folder_path.glob(\"*.csv\"):\n",
    "    df = pd.read_csv(csv_file, low_memory=False)\n",
    "    file_name = re.search(r'[^\\\\]+(?=\\.\\w+$)', str(csv_file)).group()\n",
    "\n",
    "\n",
    "    documents = df.apply(lambda row: LlamaDocument(\n",
    "        content=row.to_dict(),\n",
    "        metadata={\n",
    "            \"table_name\": file_name,\n",
    "            \"table_shape\": str(df.shape),\n",
    "            \"source\": file_name\n",
    "        }\n",
    "    ), axis=1).tolist()\n",
    "\n",
    "    # Add documents to the list\n",
    "    all_documents.extend(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1728833"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a shorter documents list for testing \n",
    "all_documents = all_documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(all_documents)):    \n",
    "    print(all_documents[i].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama_index.core.schema.Document"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(all_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector db\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"./sbm_chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"submariner\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Value for metadata table_shape must be one of (str, int, float, None)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[121]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m model = embed\n\u001b[32m      3\u001b[39m pipeline = IngestionPipeline(\n\u001b[32m      4\u001b[39m     transformations=[\n\u001b[32m      5\u001b[39m         embed,\n\u001b[32m      6\u001b[39m     ],\n\u001b[32m      7\u001b[39m     vector_store=vector_store\n\u001b[32m      8\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m nodes = \u001b[38;5;28;01mawait\u001b[39;00m pipeline.arun(documents=all_documents) \u001b[38;5;66;03m# Single threaded approximate 48 hours to run\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\RAG_PoC\\sbm_rag\\sbm_venv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:368\u001b[39m, in \u001b[36mDispatcher.span.<locals>.async_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;28mself\u001b[39m.span_enter(\n\u001b[32m    361\u001b[39m     id_=id_,\n\u001b[32m    362\u001b[39m     bound_args=bound_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    365\u001b[39m     tags=tags,\n\u001b[32m    366\u001b[39m )\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    370\u001b[39m     \u001b[38;5;28mself\u001b[39m.event(SpanDropEvent(span_id=id_, err_str=\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\RAG_PoC\\sbm_rag\\sbm_venv\\Lib\\site-packages\\llama_index\\core\\ingestion\\pipeline.py:747\u001b[39m, in \u001b[36mIngestionPipeline.arun\u001b[39m\u001b[34m(self, show_progress, documents, nodes, cache_collection, in_place, store_doc_text, num_workers, **kwargs)\u001b[39m\n\u001b[32m    745\u001b[39m     nodes_with_embeddings = [n \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes \u001b[38;5;28;01mif\u001b[39;00m n.embedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    746\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nodes_with_embeddings:\n\u001b[32m--> \u001b[39m\u001b[32m747\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.vector_store.async_add(nodes_with_embeddings)\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m nodes\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\RAG_PoC\\sbm_rag\\sbm_venv\\Lib\\site-packages\\llama_index\\core\\vector_stores\\types.py:374\u001b[39m, in \u001b[36mBasePydanticVectorStore.async_add\u001b[39m\u001b[34m(self, nodes, **kwargs)\u001b[39m\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34masync_add\u001b[39m(\n\u001b[32m    365\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    366\u001b[39m     nodes: Sequence[BaseNode],\n\u001b[32m    367\u001b[39m     **kwargs: Any,\n\u001b[32m    368\u001b[39m ) -> List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    369\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    370\u001b[39m \u001b[33;03m    Asynchronously add nodes to vector store.\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[33;03m    NOTE: this is not implemented for all vector stores. If not implemented,\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[33;03m    it will just call add synchronously.\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\RAG_PoC\\sbm_rag\\sbm_venv\\Lib\\site-packages\\llama_index\\vector_stores\\chroma\\base.py:285\u001b[39m, in \u001b[36mChromaVectorStore.add\u001b[39m\u001b[34m(self, nodes, **add_kwargs)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m node_chunk:\n\u001b[32m    284\u001b[39m     embeddings.append(node.get_embedding())\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     metadata_dict = \u001b[43mnode_to_metadata_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mflat_metadata\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m metadata_dict:\n\u001b[32m    289\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m metadata_dict[key] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\RAG_PoC\\sbm_rag\\sbm_venv\\Lib\\site-packages\\llama_index\\core\\vector_stores\\utils.py:43\u001b[39m, in \u001b[36mnode_to_metadata_dict\u001b[39m\u001b[34m(node, remove_text, text_field, flat_metadata)\u001b[39m\n\u001b[32m     40\u001b[39m metadata: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = node_dict.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m flat_metadata:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[43m_validate_is_flat_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# store entire node as json string - some minor text duplication\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remove_text:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\RAG_PoC\\sbm_rag\\sbm_venv\\Lib\\site-packages\\llama_index\\core\\vector_stores\\utils.py:27\u001b[39m, in \u001b[36m_validate_is_flat_dict\u001b[39m\u001b[34m(metadata_dict)\u001b[39m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mMetadata key must be str!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m))):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     28\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mValue for metadata \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be one of (str, int, float, None)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Value for metadata table_shape must be one of (str, int, float, None)"
     ]
    }
   ],
   "source": [
    "model = embed\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        embed,\n",
    "    ],\n",
    "    vector_store=vector_store\n",
    ")\n",
    "\n",
    "nodes = await pipeline.arun(documents=all_documents) # Single threaded approximate 48 hours to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "print(len(nodes[0].embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nest_asyncio.apply()\n",
    "\n",
    "# # Define batch size\n",
    "# batch_size = 250  # You may need to tune this based on your memory constraints and API rate limits\n",
    "\n",
    "# # Create batches\n",
    "# batches = [all_documents[i: i + batch_size] for i in range(0, len(all_documents), batch_size)]\n",
    "\n",
    "# async def process_documents():\n",
    "#     # Create pipeline\n",
    "#     pipeline = IngestionPipeline(\n",
    "#         transformations=[\n",
    "#             MistralAIEmbedding()\n",
    "#         ]\n",
    "#     )\n",
    "    \n",
    "#     # Process batches concurrently with controlled concurrency\n",
    "#     all_nodes = []\n",
    "#     semaphore = asyncio.Semaphore(10)  # Limit concurrent requests to avoid API rate limits\n",
    "    \n",
    "#     async def process_batch_with_semaphore(batch_idx, batch):\n",
    "#         async with semaphore:\n",
    "#             try:\n",
    "#                 result = await pipeline.arun(documents=batch)\n",
    "#                 return result\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing batch {batch_idx}: {e}\")\n",
    "#                 return []\n",
    "    \n",
    "#     # Create tasks for all batches\n",
    "#     tasks = [process_batch_with_semaphore(i, batch) for i, batch in enumerate(batches)]\n",
    "    \n",
    "#     # Process results as they complete\n",
    "#     for result in await tqdm_asyncio.gather(*tasks, desc=\"Processing batches\"):\n",
    "#         all_nodes.extend(result)\n",
    "    \n",
    "#     return all_nodes\n",
    "\n",
    "# # Run the processing\n",
    "# nodes = await process_documents()\n",
    "# print(f\"Processed {len(nodes)} nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nest_asyncio.apply()\n",
    "\n",
    "# batch_size = 250\n",
    "\n",
    "# batches = [all_documents[i: i + batch_size] for i in range(0, len(all_documents), batch_size)]\n",
    "\n",
    "# save_dir = Path(\"intermediate_embeddings\")\n",
    "# save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# async def process_batch(batch):\n",
    "    \n",
    "#     # Create pipeline for this batch\n",
    "#     pipeline = IngestionPipeline(\n",
    "#         transformations=[\n",
    "#             MistralAI(model=embed)\n",
    "#         ]\n",
    "#     )\n",
    "#     return await pipeline.arun(documents=batch)\n",
    "\n",
    "# async def main():\n",
    "#     nodes = []\n",
    "    \n",
    "#     # Create semaphore to limit concurrent API calls\n",
    "#     semaphore = asyncio.Semaphore(3)  # Adjust based on API rate limits\n",
    "    \n",
    "#     async def bounded_process_batch(batch_idx, batch):\n",
    "#         async with semaphore:\n",
    "#             try:\n",
    "#                 result = await process_batch(batch)\n",
    "#                 print(f\"Batch {batch_idx}/{len(batches)} completed with {len(result)} nodes\")\n",
    "#                 return result\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error processing batch {batch_idx}: {e}\")\n",
    "#                 return []\n",
    "    \n",
    "#     # Create tasks\n",
    "#     tasks = [bounded_process_batch(i, batch) for i, batch in enumerate(batches)]\n",
    "    \n",
    "#     # Use as_completed to process results as they finish\n",
    "#     for i, future in enumerate(tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Processing batches\")):\n",
    "#         result = await future\n",
    "#         nodes.extend(result)\n",
    "        \n",
    "#         # Save intermediate results every 10 batches (adjust as needed)\n",
    "#         if i > 0 and i % 10 == 0:\n",
    "#             print(f\"Saving intermediate results ({i}/{len(tasks)} batches processed)...\")\n",
    "#             with open(save_dir / f\"nodes_checkpoint_{i}.pkl\", \"wb\") as f:\n",
    "#                 pickle.dump(nodes, f)\n",
    "    \n",
    "#     # Save final results\n",
    "#     with open(save_dir / \"nodes_final.pkl\", \"wb\") as f:\n",
    "#         pickle.dump(nodes, f)\n",
    "    \n",
    "#     print(f\"Processed {len(nodes)} nodes\")\n",
    "#     return nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes = await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 5\n",
      "Number of batches: 1\n",
      "Batch size: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'datetime' has no attribute 'now'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[108]\u001b[39m\u001b[32m, line 112\u001b[39m\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m all_results, batch_stats\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m embedded_docs, batch_stats = \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[108]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m tqdm(concurrent.futures.as_completed(future_to_batch), total=\u001b[38;5;28mlen\u001b[39m(batches), desc=\u001b[33m\"\u001b[39m\u001b[33mProcessing batches\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     74\u001b[39m     batch_idx = future_to_batch[future]\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     start_time = \u001b[43mdatetime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnow\u001b[49m()\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     77\u001b[39m         results = future.result()\n",
      "\u001b[31mAttributeError\u001b[39m: module 'datetime' has no attribute 'now'"
     ]
    }
   ],
   "source": [
    "## Test with mistral documenations ressources\n",
    "\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import os\n",
    "from mistralai import Mistral\n",
    "import concurrent.futures\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load environment variables\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "mistral_client = Mistral(api_key=MISTRAL_API_KEY)\n",
    "model = \"mistral-embed\"\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 100  # Adjust based on API constraints\n",
    "save_dir = Path(\"intermediate_embeddings\")\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Function to extract content from documents for embedding\n",
    "def get_document_texts(docs):\n",
    "    return [doc.get_content() for doc in docs[:10]]\n",
    "\n",
    "# Function to create embeddings using Mistral API\n",
    "def create_embeddings_batch(batch_docs):\n",
    "    try:\n",
    "        batch_texts = get_document_texts(batch_docs)\n",
    "        response = mistral_client.embeddings.create(\n",
    "            model=\"mistral-embed\",\n",
    "            inputs=batch_texts,\n",
    "        )\n",
    "        \n",
    "        # Create dictionary with document and its embedding\n",
    "        results = []\n",
    "        for i, doc in enumerate(batch_docs):\n",
    "            embedding = response.data[i].embedding\n",
    "            results.append({\n",
    "                \"document\": doc,\n",
    "                \"embedding\": embedding,\n",
    "                \"metadata\": doc.get_metadata()\n",
    "            })\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating embeddings: {e}\")\n",
    "        return []\n",
    "\n",
    "batch_stats = []\n",
    "\n",
    "# Modify your main() function to collect statistics\n",
    "async def main():\n",
    "    # Create batches\n",
    "    batches = [all_documents[i: i + batch_size] for i in range(0, len(all_documents), batch_size)]\n",
    "    all_results = []\n",
    "    \n",
    "    print(f\"Total documents: {len(all_documents)}\")\n",
    "    print(f\"Number of batches: {len(batches)}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    \n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n",
    "        # Submit all batch processing tasks\n",
    "        future_to_batch = {\n",
    "            executor.submit(create_embeddings_batch, batch): i \n",
    "            for i, batch in enumerate(batches)\n",
    "        }\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in tqdm(concurrent.futures.as_completed(future_to_batch), total=len(batches), desc=\"Processing batches\"):\n",
    "            batch_idx = future_to_batch[future]\n",
    "            start_time = datetime.now()\n",
    "            try:\n",
    "                results = future.result()\n",
    "                end_time = datetime.now()\n",
    "                processing_time = (end_time - start_time).total_seconds()\n",
    "                \n",
    "                # Collect batch statistics\n",
    "                doc_lengths = [len(get_document_texts([doc.document])[0]) for doc in results]\n",
    "                batch_stats.append({\n",
    "                    'batch_idx': batch_idx,\n",
    "                    'batch_size': len(results),\n",
    "                    'avg_doc_length': np.mean(doc_lengths) if doc_lengths else 0,\n",
    "                    'max_doc_length': max(doc_lengths) if doc_lengths else 0,\n",
    "                    'min_doc_length': min(doc_lengths) if doc_lengths else 0,\n",
    "                    'processing_time': processing_time,\n",
    "                    'timestamp': end_time\n",
    "                })\n",
    "                \n",
    "                all_results.extend(results)\n",
    "                print(f\"Batch {batch_idx}/{len(batches)} completed with {len(results)} embeddings\")\n",
    "                \n",
    "                # Save intermediate results every 10 batches\n",
    "                if batch_idx > 0 and batch_idx % 10 == 0:\n",
    "                    print(f\"Saving intermediate results ({batch_idx}/{len(batches)} batches processed)...\")\n",
    "                    with open(save_dir / f\"embeddings_checkpoint_{batch_idx}.pkl\", \"wb\") as f:\n",
    "                        pickle.dump(all_results, f)\n",
    "            except Exception as e:\n",
    "                print(f\"Batch {batch_idx} generated an exception: {e}\")\n",
    "    \n",
    "    # Save final results\n",
    "    with open(save_dir / \"embeddings_final.pkl\", \"wb\") as f:\n",
    "        pickle.dump(all_results, f)\n",
    "    \n",
    "    print(f\"Created embeddings for {len(all_results)} documents\")\n",
    "    return all_results, batch_stats\n",
    "\n",
    "# Run the main function\n",
    "embedded_docs, batch_stats = await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a DataFrame from batch statistics\n",
    "# df = pd.DataFrame(batch_stats)\n",
    "\n",
    "# # Set up the visualization\n",
    "# fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "# fig.suptitle('API Batch Processing Visualization', fontsize=16)\n",
    "\n",
    "# # Batch sizes\n",
    "# axs[0, 0].bar(df['batch_idx'], df['batch_size'])\n",
    "# axs[0, 0].set_title('Documents per Batch')\n",
    "# axs[0, 0].set_xlabel('Batch Index')\n",
    "# axs[0, 0].set_ylabel('Number of Documents')\n",
    "\n",
    "# # Document lengths\n",
    "# axs[0, 1].plot(df['batch_idx'], df['avg_doc_length'], 'b-', label='Avg Length')\n",
    "# axs[0, 1].fill_between(df['batch_idx'], \n",
    "#                        df['min_doc_length'], \n",
    "#                        df['max_doc_length'], \n",
    "#                        alpha=0.2, \n",
    "#                        color='blue')\n",
    "# axs[0, 1].set_title('Document Length per Batch')\n",
    "# axs[0, 1].set_xlabel('Batch Index')\n",
    "# axs[0, 1].set_ylabel('Character Count')\n",
    "# axs[0, 1].legend()\n",
    "\n",
    "# # Processing time\n",
    "# axs[1, 0].bar(df['batch_idx'], df['processing_time'])\n",
    "# axs[1, 0].set_title('Processing Time per Batch')\n",
    "# axs[1, 0].set_xlabel('Batch Index')\n",
    "# axs[1, 0].set_ylabel('Time (seconds)')\n",
    "\n",
    "# # Cumulative documents processed over time\n",
    "# if len(df) > 1:\n",
    "#     df = df.sort_values('timestamp')\n",
    "#     df['cumulative_docs'] = df['batch_size'].cumsum()\n",
    "#     axs[1, 1].plot(range(len(df)), df['cumulative_docs'])\n",
    "#     axs[1, 1].set_title('Cumulative Documents Processed')\n",
    "#     axs[1, 1].set_xlabel('Batch (in order of completion)')\n",
    "#     axs[1, 1].set_ylabel('Total Documents')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.subplots_adjust(top=0.9)\n",
    "# plt.show()\n",
    "\n",
    "# # Print summary statistics\n",
    "# print(\"Summary Statistics:\")\n",
    "# print(f\"Total batches processed: {len(df)}\")\n",
    "# print(f\"Total documents processed: {df['batch_size'].sum()}\")\n",
    "# print(f\"Average documents per batch: {df['batch_size'].mean():.2f}\")\n",
    "# print(f\"Average processing time per batch: {df['processing_time'].mean():.2f} seconds\")\n",
    "# print(f\"Total processing time: {df['processing_time'].sum():.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the directory where the pickle files are saved\n",
    "# save_dir = Path(\"intermediate_embeddings\")\n",
    "\n",
    "# # Read a specific checkpoint file\n",
    "# def read_checkpoint(checkpoint_number):\n",
    "#     checkpoint_path = save_dir / f\"nodes_checkpoint_{checkpoint_number}.pkl\"\n",
    "#     try:\n",
    "#         with open(checkpoint_path, \"rb\") as f:\n",
    "#             nodes = pickle.load(f)\n",
    "#         print(f\"Loaded {len(nodes)} nodes from checkpoint {checkpoint_number}\")\n",
    "#         return nodes\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"Checkpoint file not found: {checkpoint_path}\")\n",
    "#         return None\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading checkpoint {checkpoint_number}: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Read the final results file\n",
    "# def read_final_results():\n",
    "#     final_path = save_dir / \"nodes_final.pkl\"\n",
    "#     try:\n",
    "#         with open(final_path, \"rb\") as f:\n",
    "#             nodes = pickle.load(f)\n",
    "#         print(f\"Loaded {len(nodes)} nodes from final results\")\n",
    "#         return nodes\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"Final results file not found: {final_path}\")\n",
    "#         return None\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading final results: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Example usage:\n",
    "# # To read a specific checkpoint (e.g., checkpoint 20):\n",
    "# nodes_checkpoint_20 = read_checkpoint(20)\n",
    "\n",
    "# # To read the final results:\n",
    "# nodes_final = read_final_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for batch, i in batches:\n",
    "#     print(batch[i].get_content())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
